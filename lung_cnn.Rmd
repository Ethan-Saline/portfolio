---
title: "Lung X-Ray Diagnosis using a CNN"
output: html_document
---

```{r, warning=FALSE, message=FALSE, results='hide'}
library(reticulate)
use_condaenv("r-tf", required = TRUE)
```

```{python}
import os
import tempfile
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.utils import class_weight
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from kaggle.api.kaggle_api_extended import KaggleApi
```

```{python}
# -------------------------
# Parameters
# -------------------------
IMG_SIZE = 128
BATCH_SIZE = 32
NUM_EPOCHS = 8
NUM_CLASSES = 4
SEED = 42
```

```{python}
# -------------------------
# Stage 0: Create temporary directory
# -------------------------
tmp_dir = tempfile.TemporaryDirectory()
DATA_DIR = tmp_dir.name

# For debug
#print(f"Temporary folder: {DATA_DIR}")
```

```{python, warning=FALSE, message=FALSE, results='hide'}
# -------------------------
# Stage 1: Download & extract dataset
# -------------------------
api = KaggleApi()
api.authenticate()
dataset_name = "jtiptj/chest-xray-pneumoniacovid19tuberculosis"
api.dataset_download_files(dataset_name, path=DATA_DIR, unzip=True)

train_dir = os.path.join(DATA_DIR, "train")
val_dir   = os.path.join(DATA_DIR, "val")
test_dir  = os.path.join(DATA_DIR, "test")
```

```{python}
# -------------------------
# Stage 2: Load & preprocess datasets (fixed)
# -------------------------
def preprocess(images, labels):
    images = tf.image.rgb_to_grayscale(images)
    images = layers.Rescaling(1./255)(images)
    return images, labels

# Load datasets first
train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    labels='inferred',
    label_mode='int',
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    image_size=(IMG_SIZE, IMG_SIZE),
    shuffle=True,
    seed=SEED
)

val_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(
    val_dir,
    labels='inferred',
    label_mode='int',
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    image_size=(IMG_SIZE, IMG_SIZE),
    shuffle=False
)

test_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    labels='inferred',
    label_mode='int',
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    image_size=(IMG_SIZE, IMG_SIZE),
    shuffle=False
)

# Save class names BEFORE any preprocessing or caching
class_names = train_dataset_raw.class_names

# Apply preprocessing and caching
train_dataset = train_dataset_raw.map(preprocess).cache()
val_dataset   = val_dataset_raw.map(preprocess).cache()
test_dataset  = test_dataset_raw.map(preprocess).cache()
```

```{python}
# -------------------------
# Stage 3: Build CNN model
# -------------------------
def build_model():
    model = models.Sequential([
        layers.Input(shape=(IMG_SIZE, IMG_SIZE, 1)),
        layers.Conv2D(128, 3, activation='gelu'),
        layers.MaxPooling2D(3),
        layers.Conv2D(64, 3, activation='gelu'),
        layers.MaxPooling2D(2),
        layers.Conv2D(16, 3, activation='gelu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(3),
        layers.Flatten(),
        layers.Dropout(0.1),
        layers.Dense(130, activation='gelu'),
        layers.Dense(NUM_CLASSES, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

model = build_model()
```

```{python}
# -------------------------
# Stage 4: Compute class weights
# -------------------------
all_labels = np.concatenate([y for _, y in train_dataset], axis=0)
class_weights_array = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(all_labels),
    y=all_labels
)
class_weights_dict = dict(enumerate(class_weights_array))
```

```{python, results='hide'}
# -------------------------
# Stage 5: Train model (with checkpoint)
# -------------------------
checkpoint_path = os.path.join(DATA_DIR, "cnn_model.h5")
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    checkpoint_path, save_best_only=True
)

model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=NUM_EPOCHS,
    class_weight=class_weights_dict,
    callbacks=[checkpoint_callback]
)
```

```{python}
# -------------------------
# Stage 6: Evaluation
# -------------------------
# Reload best model
model = tf.keras.models.load_model(checkpoint_path)

predictions = model.predict(test_dataset)
y_pred = np.argmax(predictions, axis=1)
y_true = np.concatenate([y for _, y in test_dataset], axis=0)

print(f"\nManual accuracy: {accuracy_score(y_true, y_pred):.4f}\n")
print(classification_report(y_true, y_pred, target_names=class_names))

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_names,
    yticklabels=class_names
)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()
plt.show()
```

```{python}
# -------------------------
# Stage 7: Cleanup temp directory
# -------------------------
tmp_dir.cleanup()
```